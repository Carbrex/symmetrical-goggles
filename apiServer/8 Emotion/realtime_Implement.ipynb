{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeidanGR/SpeechEmotionRecognition_Realtime/blob/main/3_realtime_ser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA4vLC6ID715"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install pydub\n",
        "%pip install noisereduce\n",
        "%pip install pyaudio\n",
        "%pip install json-tricks\n",
        "%pip install torch\n",
        "#hello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4cVP0NmD71-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "from json_tricks import load\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import librosa\n",
        "from pydub import AudioSegment, effects\n",
        "import noisereduce as nr\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSjs9TReEUls"
      },
      "source": [
        "# **LOAD MODEL**\n",
        "\n",
        "Loading the speech emotion recognition LSTM model and weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6qXd1-vD72A",
        "outputId": "9a687758-6391-4ec5-ec83-e52bb0f5ffc9"
      },
      "outputs": [],
      "source": [
        "saved_model_path = './model8723.json'\n",
        "saved_weights_path = './model8723_weights.h5'\n",
        "\n",
        "#Reading the model from JSON file\n",
        "with open(saved_model_path, 'r') as json_file:\n",
        "    json_savedModel = json_file.read()\n",
        "    \n",
        "# Loading the model architecture, weights\n",
        "model = tf.keras.models.model_from_json(json_savedModel)\n",
        "model.load_weights(saved_weights_path)\n",
        "\n",
        "# Compiling the model with similar parameters as the original model.\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='RMSProp', \n",
        "                metrics=['categorical_accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyCwGZaeD72C"
      },
      "outputs": [],
      "source": [
        "def preprocess(file_path, frame_length = 2048, hop_length = 512):\n",
        "    '''\n",
        "    A process to an audio .wav file before execcuting a prediction.\n",
        "      Arguments:\n",
        "      - file_path - The system path to the audio file.\n",
        "      - frame_length - Length of the frame over which to compute the speech features. default: 2048\n",
        "      - hop_length - Number of samples to advance for each frame. default: 512\n",
        "\n",
        "      Return:\n",
        "        'X_3D' variable, containing a shape of: (batch, timesteps, feature) for a single file (batch = 1).\n",
        "    ''' \n",
        "    # Fetch sample rate.\n",
        "    _, sr = librosa.load(path = file_path, sr = None)\n",
        "    # Load audio file\n",
        "    rawsound = AudioSegment.from_file(file_path, duration = None) \n",
        "    # Normalize to 5 dBFS \n",
        "    normalizedsound = effects.normalize(rawsound, headroom = 5.0) \n",
        "    # Transform the audio file to np.array of samples\n",
        "    normal_x = np.array(normalizedsound.get_array_of_samples(), dtype = 'float32') \n",
        "    # Noise reduction                  \n",
        "    final_x = nr.reduce_noise(normal_x, sr=sr)\n",
        "        \n",
        "        \n",
        "    f1 = librosa.feature.rms(y=final_x, frame_length=frame_length, hop_length=hop_length, center=True, pad_mode='reflect').T # Energy - Root Mean Square\n",
        "    f2 = librosa.feature.zero_crossing_rate(final_x, frame_length=frame_length, hop_length=hop_length,center=True).T # ZCR\n",
        "    f3 = librosa.feature.mfcc(y=final_x, sr=sr, S=None, n_mfcc=13, hop_length = hop_length).T # MFCC   \n",
        "    X = np.concatenate((f1, f2, f3), axis = 1)\n",
        "    \n",
        "    X_3D = np.expand_dims(X, axis=0)\n",
        "    \n",
        "    return X_3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXfNZp8qD72D"
      },
      "outputs": [],
      "source": [
        "# Emotions list is created for a readable form of the model prediction.\n",
        "\n",
        "emotions = {\n",
        "    0 : 'neutral',\n",
        "    1 : 'calm',\n",
        "    2 : 'happy',\n",
        "    3 : 'sad',\n",
        "    4 : 'angry',\n",
        "    5 : 'fearful',\n",
        "    6 : 'disgust',\n",
        "    7 : 'suprised'   \n",
        "}\n",
        "emo_list = list(emotions.values())\n",
        "\n",
        "def is_silent(data):\n",
        "    # Returns 'True' if below the 'silent' threshold\n",
        "    return max(data) < 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s4DyZckD72E",
        "outputId": "862921df-ad36-4b23-9532-81762f128793"
      },
      "outputs": [],
      "source": [
        "import pyaudio\n",
        "import wave\n",
        "from array import array\n",
        "import struct\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Initialize variables\n",
        "RATE = 24414\n",
        "CHUNK = 512\n",
        "RECORD_SECONDS = 7.1\n",
        "\n",
        "FORMAT = pyaudio.paInt32\n",
        "CHANNELS = 1\n",
        "WAVE_OUTPUT_FILE = \"./output.wav\"\n",
        "\n",
        "# Open an input channel\n",
        "p = pyaudio.PyAudio()\n",
        "stream = p.open(format=FORMAT,\n",
        "                channels=CHANNELS,\n",
        "                rate=RATE,\n",
        "                input=True,\n",
        "                frames_per_buffer=CHUNK)\n",
        "\n",
        "\n",
        "# Initialize a non-silent signals array to state \"True\" in the first 'while' iteration.\n",
        "data = array('h', np.random.randint(size = 512, low = 0, high = 500))\n",
        "\n",
        "# SESSION START\n",
        "print(\"** session started\")\n",
        "total_predictions = [] # A list for all predictions in the session.\n",
        "tic = time.perf_counter()\n",
        "\n",
        "while is_silent(data) == False:\n",
        "    print(\"* recording...\")\n",
        "    frames = [] \n",
        "    data = np.nan # Reset 'data' variable.\n",
        "\n",
        "    timesteps = int(RATE / CHUNK * RECORD_SECONDS) # => 339\n",
        "\n",
        "    # Insert frames to 'output.wav'.\n",
        "    for i in range(0, timesteps):\n",
        "        data = array('l', stream.read(CHUNK)) \n",
        "        frames.append(data)\n",
        "\n",
        "        wf = wave.open(WAVE_OUTPUT_FILE, 'wb')\n",
        "        wf.setnchannels(CHANNELS)\n",
        "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
        "        wf.setframerate(RATE)\n",
        "        wf.writeframes(b''.join(frames))\n",
        "\n",
        "    print(\"* done recording\")\n",
        "\n",
        "    x = preprocess(WAVE_OUTPUT_FILE) # 'output.wav' file preprocessing.\n",
        "    # Model's prediction => an 8 emotion probabilities array.\n",
        "    #make http request to predict model at port 8051\n",
        "   \n",
        "    url = 'http://localhost:8501/v1/models/fashion_model:predict'\n",
        "    headers = {'content-type': 'application/json', 'Accept-Charset': 'UTF-8'}\n",
        "    r = requests.post(url, data=json.dumps({\"inputs\":{\"lstm_input\": x.tolist()}}), headers=headers)\n",
        "    if(r.status_code != 200):\n",
        "        print(\"error\")\n",
        "        break\n",
        "    result = r.json()\n",
        "    print(result)\n",
        "    predictions = np.array(result['outputs'])\n",
        "\n",
        "    # predictions = model.predict(x, use_multiprocessing=True)\n",
        "    pred_list = list(predictions)\n",
        "    pred_np = np.squeeze(np.array(pred_list).tolist(), axis=0) # Get rid of 'array' & 'dtype' statments.\n",
        "    total_predictions.append(pred_np)\n",
        "    \n",
        "    # Present emotion distribution for a sequence (7.1 secs).\n",
        "    fig = plt.figure(figsize = (10, 2))\n",
        "    plt.bar(emo_list, pred_np, color = 'darkturquoise')\n",
        "    plt.ylabel(\"Probabilty (%)\")\n",
        "    plt.show()\n",
        "    \n",
        "    max_emo = np.argmax(predictions)\n",
        "    print('max emotion:', emotions.get(max_emo,-1))\n",
        "    \n",
        "    print(100*'-')\n",
        "    \n",
        "    # Define the last 2 seconds sequence.\n",
        "    last_frames = np.array(struct.unpack(str(96 * CHUNK) + 'B' , np.stack(( frames[-1], frames[-2], frames[-3], frames[-4],\n",
        "                                                                            frames[-5], frames[-6], frames[-7], frames[-8],\n",
        "                                                                            frames[-9], frames[-10], frames[-11], frames[-12],\n",
        "                                                                            frames[-13], frames[-14], frames[-15], frames[-16],\n",
        "                                                                            frames[-17], frames[-18], frames[-19], frames[-20],\n",
        "                                                                            frames[-21], frames[-22], frames[-23], frames[-24]),\n",
        "                                                                            axis =0)) , dtype = 'b')\n",
        "    if is_silent(last_frames): # If the last 2 seconds are silent, end the session.\n",
        "        break\n",
        "\n",
        "# SESSION END        \n",
        "toc = time.perf_counter()\n",
        "stream.stop_stream()\n",
        "stream.close()\n",
        "p.terminate()\n",
        "wf.close()\n",
        "print('** session ended')\n",
        "\n",
        "# Present emotion distribution for the whole session.\n",
        "total_predictions_np =  np.mean(np.array(total_predictions).tolist(), axis=0)\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "plt.bar(emo_list, total_predictions_np, color = 'indigo')\n",
        "plt.ylabel(\"Mean probabilty (%)\")\n",
        "plt.title(\"Session Summary\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Emotions analyzed for: {(toc - tic):0.4f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "3_realtime_ser.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
